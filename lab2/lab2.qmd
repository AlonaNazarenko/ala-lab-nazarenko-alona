---
title: "Lab 2: Applications of Eigenvectors"
subtitle: "MATH 252: Applied Linear Algebra"
author: 
  - name: "Alona Nazarenko"
    email: [anazarenko1@kse.org.ua]
    affiliations: ["Kyiv School of Economics"]
format:
  html: 
    highlight-style: github
    theme: [default, ../custom.scss]
---

## Introduction

In this laboratory work, I explored practical applications of **eigenvalues and eigenvectors** in three different domains:

1. **Computing eigenvalues and eigenvectors** of matrices
2. **Image compression using Principal Component Analysis (PCA)**
3. **Cryptography through matrix diagonalization**

---

## Library Setup

For this lab, I'm using **NumPy** for numerical computations, **Matplotlib** for visualization

```{python}
#| label: import_library
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
```

---

## Task 1: Computing Eigenvalues and Eigenvectors

### Theory

For a square matrix **A**, an **eigenvector** **v** and its corresponding **eigenvalue** Œª satisfy the equation:

$$
A \cdot v = \lambda \cdot v
$$

This means that when we apply the transformation **A** to vector **v**, the result is simply **v** scaled by Œª.

I also add a check for complex eigen values because in one my ocasion testing matrix has two complex eigenvalues but after `np.real` it became the same one and iddnt pass the check

### Implementation

```{python}
#| label: task_1_def
def task_1 (A):
    eigvals, eigvecs = np.linalg.eig(A)
    for i, lam in enumerate(eigvals):
        if lam.imag != 0:
            print(f"Eigenvalue {lam} has non-zero imaginary part")
        else:
            print(f"Eigenvalue {lam} is real")
    eigvals = eigvals.real
    eigvecs = eigvecs.real
    print("Eigenvalues:", eigvals)
    print("\nEigenvectors:", eigvecs)

    print("\nVerification A¬∑v = Œª¬∑v:")
    for i in range(len(eigvals)):
        v = eigvecs[:, i]
        s = eigvals[i]

        left = A @ v
        right = s * v

        if np.allclose(left, right):
            print(f"‚úì Eigenpair {i+1} verified.")
        else:
            print(f"‚úó Eigenpair {i+1} FAILED.")
            print("A¬∑v =", left)
            print("Œª¬∑v =", right)
    return (eigvals, eigvecs)

```

### Example Usage
```{python}
#| label: def_random
#| include: false

def random_int_matrix(n, low=-10, high=10):
    return np.random.randint(low, high+1, size=(n, n))

np.random.seed(27)
k = random_int_matrix(2)
```

```{python}
#| label: def_display_matrix
#| include: false
from IPython.display import display, Math

def matrix_to_latex(M, name="A"):
    rows = [" & ".join(map(str, row)) for row in M]
    body = r"\\ ".join(rows)
    return rf"{name} = \begin{{bmatrix}} {body} \end{{bmatrix}}"
```

```{python}
#| label: k_matrix
#| echo: false
k_latex = matrix_to_latex(k, "K")
display(Math(k_latex))
```

```{python}
#| label: example_task1

task_1(k)
```

### Results and Analysis

The function successfully computes eigenvalues and eigenvectors, and the verification confirms that **A¬∑v = Œª¬∑v** holds for each pair. This validates that our computed eigenvalues and eigenvectors are correct.

---

## Task 2: Image Compression Using PCA

### Theory: What is PCA?

**Principal Component Analysis (PCA)** is a dimensionality reduction technique that identifies the most important patterns in data. For images:
- Each pixel is treated as a feature
- PCA finds directions (principal components) that capture maximum variance
- By keeping only the top components, we can compress the image while preserving its essential structure

### Diving deeper

Also, I was interested to dive deeper into this topic and write it from scratch in NumPy.  
Thanks to this [site](https://dou.ua/forums/topic/47053/), where I got to know how eigenvectors are related to PCA, and I was also surprised that the covariance matrix‚Äôs eigenvectors construct an orthonormal matrix.


### Step 1: Load, Display Original Image & Convert to Grayscale

```{python}
#| label: image
#| code-fold: true
img = imread("china.jpg")
img_sum = img.sum(axis=2)
img_bw = img_sum/img_sum.max()
plt.imshow(img_bw, cmap='gray')
```

### Step 2: Apply PCA and Find Components for 95% Variance

```{python}
#| label: my_data_class
#| include: false

class MyData:
    def __init__(self, mean, reduced_matrix, T_matrix, num_component, cumulative_var):
        self.mean = mean
        self.reduced_matrix = reduced_matrix
        self.T_matrix = T_matrix
        self.num_component = num_component
        self.cumulative_var = cumulative_var
        
```

```{python}
#| label: pca_function
def pca(image, k = 1, n_components = None):
    mean = np.mean(image, axis=0)
    X_centered = image - mean
    cov_matrix = np.cov(X_centered, rowvar=False)

    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    sort_index = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sort_index]
    eigenvectors = eigenvectors[:, sort_index]

    if n_components != None:
        X_reduced = X_centered @ eigenvectors[:, :n_components]
        return MyData(mean, X_reduced, eigenvectors[:, :n_components], n_components, None)

    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
    cumulative_variance = np.cumsum(explained_variance_ratio)

    num_components_k = np.argmax(cumulative_variance >= k) + 1

    X_reduced = X_centered @ eigenvectors[:, :num_components_k]
    return MyData(mean, X_reduced, eigenvectors[:, :num_components_k], num_components_k,cumulative_variance)
```
When applying PCA to the `image_bw` matrix, it becomes much clearer how each step works: first we center the data, then we compute the covariance matrix, and from that matrix we extract its eigenvalues and eigenvectors. The eigenvalues basically tell us how much variance each principal component captures, and the eigenvectors give us the new directions (axes) onto which we project the image.

After this, we can plot the cumulative explained variance and visually see how many components are needed to reach the 95% threshold. This graph clearly shows where the curve flattens out, and it makes it easy to choose the optimal number of components for reconstructing the image while keeping most of the information.
```{python}
#| label: pca_graph 
#| code-fold: true 

img_095 = pca(img_bw, 0.95)

plt.figure(figsize=(8,5))
plt.plot(np.arange(1, len(img_095.cumulative_var)+1), img_095.cumulative_var, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance threshold')
plt.axvline(x=img_095.num_component +1 , color='g', linestyle='--', label=f'{img_095.num_component +1 } components')
plt.xlabel('Number of principal components')
plt.ylabel('Cumulative variance')
plt.title('PCA: Cumulative variance explained')
plt.legend()
plt.grid(True)
plt.show()
```

### Step 3: Reconstruct Image with 95% Components

```{python}
#| label: def_reconstract 
def reconstraction (img_k):
    plt.imshow((img_k.reduced_matrix @ img_k.T_matrix.T) + img_k.mean, cmap='gray')
    plt.title(f"Components: {img_k.num_component}")
```
```{python}
#| label: reconstract_095
reconstraction(img_095)
```


### Analysis

With 95% variance coverage, the reconstructed image captures all major elements and structure. The main objects are clearly identifiable. What's missing is **sharpness** ‚Äî the fine details and high-frequency information that make an image crisp and visually appealing.

### Step 5: Compare Different Numbers of Components

```{python}
#| label: comparizon_pca
#| code-folf: true 
components_list = [5, 15, 25, 75, 100, 170]

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for ax, k in zip(axes.flatten(), components_list):
    img_k = pca(img_bw, n_components=k) 

   
    recon = (img_k.reduced_matrix @ img_k.T_matrix.T) + img_k.mean
    recon = recon.reshape(img_bw.shape)  

    ax.imshow(recon, cmap='gray')
    ax.set_title(f"Components: {k}")
    ax.axis('off')

plt.tight_layout()
plt.show()
```


### Results and Conclusions

- **Fewer components** (5-25): Image is blurry, loses significant detail, but main shapes are visible
- **Medium components** (75-100): Good balance between compression and quality
- **More components** (>100): Sharper image, captures fine details, approaching original quality
- **Optimal components** (95% variance): Provides excellent compression while maintaining recognizability

**Key insight:** PCA achieves significant dimensionality reduction ‚Äî we can represent the image with far fewer components than original dimensions while preserving essential visual information.

---

## Task 3: Cryptography Using Matrix Diagonalization

### Theory

Matrix diagonalization provides a powerful tool for encryption:
1. A **key matrix** K is used to encrypt messages
2. The encryption uses eigenvalue decomposition: **K = PDP‚Åª¬π**
3. Decryption reverses the process using the inverse transformation

### Encryption Function

```{python}
#| label: encrypt_def
def encrypt_message(message, key_matrix):
    message_vector = np.array([ord(char) for char in message])
    eigenvalues, eigenvectors = np.linalg.eig(key_matrix)
    diagonalized_key_matrix = np.dot(np.dot(eigenvectors, np.diag(eigenvalues)), np.linalg.inv(eigenvectors))
    encrypted_vector = np.dot(diagonalized_key_matrix, message_vector)
    return encrypted_vector
```

And here is the usage 
```{python}
#| label: usage_encripting 
message = "I have done it in one day & survived !!!!!"
key_matrix = np.random.randint(0, 256, (len(message), len(message)))
f = encrypt_message(message, key_matrix)
f
```

### Decryption Function

```{python}
#| label: decripting_def
def decrypt_message (encrypted_vector, key_matrix):
    decrypted_vector = np.linalg.inv(key_matrix) @ encrypted_vector
    return "".join([chr(int(round(i.real))) for i in decrypted_vector])
```

```{python}
#| label: decripting usage 
decrypt_message(f, key_matrix)
```


### Results and Analysis

The implementation successfully:
- **Encrypts** the message into a numerical vector using matrix multiplication
- **Decrypts** the encrypted vector back to the original text using inverse diagonalization
- **Verifies** that the decrypted message matches the original

The small imaginary components in the encrypted vector (e.g., `+1.04957181e-11j`) are due to floating-point precision and are effectively zero ‚Äî they're handled by taking the real part during decryption.

**Security note:** This is a simplified demonstration. Real cryptographic systems use much more sophisticated techniques to ensure security.

---

## Theoretical Questions

### 1. What are eigenvalues and eigenvectors of a matrix? How are they computed?

**Eigenvalues** (Œª) and **eigenvectors** (v) of a square matrix **A** satisfy:
$$A \cdot v = \lambda \cdot v$$

This means applying transformation **A** to **v** simply scales it by Œª without changing its direction.

**Computation:**
1. Solve the characteristic equation: **det(A - ŒªI) = 0**
2. Find eigenvalues Œª
3. For each Œª, solve **(A - ŒªI)v = 0** to find eigenvectors

In practice, we use numerical algorithms (like NumPy's `np.linalg.eig()`) that employ sophisticated methods such as QR decomposition.

### 2. What properties do eigenvectors of symmetric matrices have?

Symmetric matrices (**A = A·µÄ**) have special properties:
- All **eigenvalues are real**
- **Eigenvectors are orthogonal** (perpendicular to each other)
- The matrix can be diagonalized as: **A = QŒõQ·µÄ** where Q is orthogonal
- This makes them ideal for PCA and many optimization problems

### 3. What can be the disadvantages of using PCA, and what strategies can be used to overcome these disadvantages?

**Disadvantages:**

1. **Loss of interpretability** ‚Äî principal components are linear combinations of original features

2. **Assumes linearity** ‚Äî may not capture complex nonlinear relationships

3. **Sensitive to scaling** ‚Äî features must be standardized
4. **Variance ‚â† importance** ‚Äî high-variance components aren't always most meaningful

**Strategies to overcome:**

- **Feature scaling:** Standardize data before applying PCA
- **Kernel PCA:** Use kernel methods for nonlinear dimensionality reduction
- **Sparse PCA:** Add sparsity constraints for better interpretability
- **Domain knowledge:** Combine PCA with expert understanding of the problem
- **Alternative methods:** Use autoencoders, t-SNE, or UMAP for complex data

### 4. What advantages does matrix diagonalization have in cryptography? How is it applied for encryption and decryption?

**Advantages:**

1. **Invertibility:** Diagonalization ensures the key matrix can be easily inverted
2. **Computational efficiency:** Matrix operations become simpler with diagonal matrices
3. **Mathematical foundation:** Provides rigorous theoretical basis for security

**Application:**

- **Encryption:** Message vector is multiplied by key matrix **K**
- **Decryption:** Uses **K‚Åª¬π = PD‚Åª¬πP‚Åª¬π** where K = PDP‚Åª¬π
- The eigenvalue decomposition makes inversion computationally efficient

**Note:** Modern cryptography uses more advanced techniques (RSA, ECC) but the principles of linear algebra remain fundamental.

---

## Conclusion

This laboratory work demonstrated three powerful applications of eigenvalues and eigenvectors:

1. **Direct computation** verified the fundamental equation A¬∑v = Œª¬∑v

2. **PCA for image compression** showed how dimensionality reduction preserves essential information

3. **Cryptographic applications** illustrated how matrix diagonalization enables secure communication

These examples highlight how **theoretical linear algebra concepts** translate into **practical real-world applications** across computer science, data science, and information security.

---

## üéâ The End

If you've read this far ‚Äî **congratulations!** ü•≥  
You've completed Lab 2 on **Applications of Eigenvectors**.  

Keep exploring the beautiful connections between mathematics and technology! üîç‚ú®
